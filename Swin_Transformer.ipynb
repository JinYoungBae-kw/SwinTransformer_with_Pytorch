{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 구현"
      ],
      "metadata": {
        "id": "0yirnNA3otFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP"
      ],
      "metadata": {
        "id": "ThacfTXGouUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, patches_dim, n_hidden_layer, drop_p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(patches_dim, n_hidden_layer),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(n_hidden_layer, patches_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.mlp(x)\n",
        "        return result"
      ],
      "metadata": {
        "id": "4EsYhRAszXqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PatchMerging"
      ],
      "metadata": {
        "id": "JmnfK6iMo23P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, patches_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patches_dim = patches_dim\n",
        "        self.norm = nn.LayerNorm(4*patches_dim, eps=1e-5)\n",
        "        self.compression = nn.Linear(4*patches_dim, 2*patches_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        image_h, image_w, _ = x.shape[1:]\n",
        "        padding_x = func.pad(x, (0, 0, 0, image_w % 2, 0, image_h % 2))\n",
        "\n",
        "        x0 = padding_x[..., 0::2, 0::2, :]\n",
        "        x1 = padding_x[..., 1::2, 0::2, :]\n",
        "        x2 = padding_x[..., 0::2, 1::2, :]\n",
        "        x3 = padding_x[..., 1::2, 1::2, :]\n",
        "        new_patch = torch.cat([x0, x1, x2, x3], -1)\n",
        "\n",
        "        new_patch = self.norm(new_patch)\n",
        "        result = self.compression(new_patch)\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "0lv3lgf0o0zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### W-MSA, SW-MSA"
      ],
      "metadata": {
        "id": "nmNQ8wZNpMUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShiftedWindowAttention(nn.Module):\n",
        "    def __init__(self, patches_dim, window_size, shift_size, n_heads, drop_p, device=DEVICE):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device if device is not None else torch.device(\"cpu\")\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.n_heads = n_heads\n",
        "        self.root_dk = torch.sqrt(torch.tensor(patches_dim / n_heads, dtype=torch.float32, device=self.device))\n",
        "\n",
        "        self.q_Linear = nn.Linear(patches_dim, patches_dim)\n",
        "        self.k_Linear = nn.Linear(patches_dim, patches_dim)\n",
        "        self.v_Linear = nn.Linear(patches_dim, patches_dim)\n",
        "        self.last_Linear = nn.Linear(patches_dim, patches_dim)\n",
        "        self.linear_layers = [self.q_Linear, self.k_Linear, self.v_Linear, self.last_Linear]\n",
        "\n",
        "        self.get_relative_position_bias()\n",
        "\n",
        "    def get_relative_position_bias(self):\n",
        "        B_hat = nn.Parameter(\n",
        "            torch.zeros(\n",
        "                self.n_heads,\n",
        "                (2 * self.window_size[0] - 1),\n",
        "                (2 * self.window_size[1] - 1),\n",
        "                device=self.device))\n",
        "        init.trunc_normal_(B_hat, std=0.02)\n",
        "\n",
        "        absolute_position_h = torch.arange(self.window_size[0], device=self.device)\n",
        "        absolute_position_w = torch.arange(self.window_size[1], device=self.device)\n",
        "        absolute_coordinate_h, absolute_coordinate_w = torch.meshgrid(absolute_position_h, absolute_position_w,indexing='ij')\n",
        "\n",
        "        relative_coords_h = absolute_coordinate_h.reshape(1,-1) - absolute_coordinate_h.reshape(-1,1)\n",
        "        relative_coords_w = absolute_coordinate_w.reshape(1,-1) - absolute_coordinate_w.reshape(-1,1)\n",
        "        relative_index_h = relative_coords_h + self.window_size[0] - 1\n",
        "        relative_index_w = relative_coords_w + self.window_size[1] - 1\n",
        "\n",
        "        self.B = B_hat[:, relative_index_h, relative_index_w].unsqueeze(0).unsqueeze(0).detach()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        window_h, window_w = self.window_size\n",
        "        _, H, W, _ = x.shape\n",
        "\n",
        "        pad_right = (window_w - W % window_w) % window_w\n",
        "        pad_under = (window_h - H % window_h) % window_h\n",
        "        padding_x = func.pad(x, (0, 0, 0, pad_right, 0, pad_under))\n",
        "        _, padding_h, padding_w, _ = padding_x.shape\n",
        "\n",
        "        shift_size = self.shift_size.copy()\n",
        "        if padding_h <= window_h:\n",
        "            shift_size[0] = 0\n",
        "        if padding_w <= window_w:\n",
        "            shift_size[1] = 0\n",
        "\n",
        "        if sum(shift_size) > 0:\n",
        "            shift_x = torch.roll(padding_x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n",
        "            ready_for_attention = rearrange(shift_x, 'b (n_window_h window_h) (n_window_w window_w) d -> b (n_window_h n_window_w) (window_h window_w) d',\n",
        "                                        window_h=window_h, window_w=window_w)\n",
        "        else:\n",
        "            ready_for_attention = rearrange(padding_x, 'b (n_window_h window_h) (n_window_w window_w) d -> b (n_window_h n_window_w) (window_h window_w) d',\n",
        "                                        window_h=window_h, window_w=window_w)\n",
        "\n",
        "        Q, K, V = [linear(ready_for_attention) for linear in self.linear_layers[:3]]\n",
        "        Q, K, V = [rearrange(tensor, 'b nw p (h d) -> b nw h p d', h=self.n_heads) for tensor in [Q, K, V]]\n",
        "\n",
        "        qkt_dk = torch.matmul(Q, K.transpose(-2,-1)) / self.root_dk\n",
        "        ready_for_masking = qkt_dk + self.B\n",
        "\n",
        "        if sum(shift_size) > 0:\n",
        "            window_group_number = ready_for_attention.new_zeros(padding_h, padding_w)\n",
        "            slice_h = ((0, -window_h), (-window_h, -shift_size[0]), (-shift_size[0], None))\n",
        "            slice_w = ((0, -window_w), (-window_w, -shift_size[1]), (-shift_size[1], None))\n",
        "            count = 0\n",
        "            for h in slice_h:\n",
        "                for w in slice_w:\n",
        "                    window_group_number[h[0] : h[1], w[0] : w[1]] = count\n",
        "                    count += 1\n",
        "            window_group_number = rearrange(window_group_number, '(n_window_h window_h) (n_window_w window_w) -> (n_window_h n_window_w) (window_h window_w)',\n",
        "                                            window_h=window_h, window_w=window_w)\n",
        "            mask = window_group_number.unsqueeze(2) - window_group_number.unsqueeze(1)\n",
        "            mask[mask != 0] = -1e10\n",
        "            done_masking = ready_for_masking + mask.unsqueeze(1).unsqueeze(0)\n",
        "        else:\n",
        "            done_masking = ready_for_masking\n",
        "\n",
        "        done_masking = func.softmax(done_masking, dim=-1)\n",
        "        attention_result = torch.matmul(done_masking, V)\n",
        "        concat_out = rearrange(attention_result, 'b nw h p d -> b nw p (h d)')\n",
        "        concat_out = self.last_Linear(concat_out)\n",
        "        windows_merged = rearrange(concat_out, 'b (n_window_h n_window_w) (window_h window_w) d -> b (n_window_h window_h) (n_window_w window_w) d',\n",
        "                      n_window_h=padding_h//window_h, window_h=window_h)\n",
        "\n",
        "        if sum(shift_size) > 0:\n",
        "            shift_restored = torch.roll(windows_merged, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n",
        "        else:\n",
        "            shift_restored = windows_merged\n",
        "\n",
        "        result = shift_restored[:, :H, :W, :]\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "52nYD_Weo893"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer"
      ],
      "metadata": {
        "id": "JgZgnezvqfKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, patches_dim, n_heads, window_size, shift_size, hidden_ratio, drop_p, stochastic_depth_p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.first_norm = nn.LayerNorm(patches_dim, eps=1e-5)\n",
        "        self.sw_msa = ShiftedWindowAttention(patches_dim, window_size, shift_size, n_heads, drop_p=drop_p)\n",
        "        self.second_norm = nn.LayerNorm(patches_dim, eps=1e-5)\n",
        "        self.mlp = MLP(patches_dim, int(patches_dim * hidden_ratio), drop_p=drop_p)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "        self.stochastic_depth = StochasticDepth(stochastic_depth_p, \"row\")\n",
        "\n",
        "        for linear in self.mlp.modules():\n",
        "            if isinstance(linear, nn.Linear):\n",
        "                init.xavier_uniform_(linear.weight)\n",
        "                if linear.bias is not None:\n",
        "                    init.normal_(linear.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm_out = self.first_norm(x)\n",
        "        msa_out = self.sw_msa(norm_out)\n",
        "        msa_out = self.dropout(msa_out)\n",
        "        msa_out = self.stochastic_depth(msa_out)\n",
        "        msa_result_with_skip = x + msa_out\n",
        "\n",
        "        norm_out = self.second_norm(msa_result_with_skip)\n",
        "        mlp_out = self.mlp(norm_out)\n",
        "        mlp_out = self.dropout(mlp_out)\n",
        "        mlp_out = self.stochastic_depth(mlp_out)\n",
        "        result_with_skip = msa_result_with_skip + mlp_out\n",
        "\n",
        "        return result_with_skip"
      ],
      "metadata": {
        "id": "Irgh9kNnpRj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SwinTransformer"
      ],
      "metadata": {
        "id": "HJf4lNKaqLq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self, patch_size, embedding_dim, n_transformer, n_heads, window_size, hidden_ratio, drop_p, stochastic_depth_p, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        n_all_blocks = sum(n_transformer)\n",
        "        block_index = 0\n",
        "\n",
        "        layers = []\n",
        "        layers += [nn.Sequential(nn.Conv2d(3, embedding_dim, kernel_size=(patch_size[0], patch_size[1]), stride=(patch_size[0], patch_size[1])),\n",
        "                                 Permute([0, 2, 3, 1]),\n",
        "                                 nn.LayerNorm(embedding_dim, eps=1e-5))]\n",
        "\n",
        "        for stage_num in range(len(n_transformer)):\n",
        "            stage = []\n",
        "            stage_patches_dim = embedding_dim * 2**stage_num\n",
        "\n",
        "            for transformer_block_num in range(n_transformer[stage_num]):\n",
        "                stochastic_depth_prob = stochastic_depth_p * block_index / (n_all_blocks - 1)\n",
        "\n",
        "                stage += [SwinTransformerBlock(stage_patches_dim,\n",
        "                                               n_heads[stage_num],\n",
        "                                               window_size=window_size,\n",
        "                                               shift_size=[0 if transformer_block_num % 2 == 0 else w // 2 for w in window_size],\n",
        "                                               hidden_ratio=hidden_ratio,\n",
        "                                               drop_p = drop_p,\n",
        "                                               stochastic_depth_p=stochastic_depth_prob)]\n",
        "                block_index += 1\n",
        "\n",
        "            layers += [nn.Sequential(*stage)]\n",
        "\n",
        "            if stage_num < (len(n_transformer) - 1):\n",
        "                layers += [PatchMerging(stage_patches_dim)]\n",
        "\n",
        "        self.all_layers = nn.Sequential(*layers)\n",
        "        self.norm = nn.LayerNorm(stage_patches_dim, eps=1e-5)\n",
        "        self.GAP = nn.Sequential(Permute([0, 3, 1, 2]),\n",
        "                                 nn.AdaptiveAvgPool2d((1,1)))\n",
        "        self.head = nn.Linear(stage_patches_dim, n_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.all_layers(x)\n",
        "        patches = self.norm(patches)\n",
        "        patches = self.GAP(patches)\n",
        "        patches = torch.flatten(patches, start_dim=1)\n",
        "        model_result = self.head(patches)\n",
        "\n",
        "        return model_result"
      ],
      "metadata": {
        "id": "dxJ0lMquqK_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}